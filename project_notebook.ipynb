{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('snlp': venv)"
  },
  "interpreter": {
   "hash": "f0a7245247b824f88e8cdc9749c4ec8d41cad3b6de9273672e84a39a1c855c1d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# English - Alice in wonderland\n",
    "\n",
    "## Data preparation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from importlib import reload\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pavlem/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "import preprocessor\n",
    "preprocessor = reload(preprocessor)\n",
    "\n",
    "\n",
    "pp = preprocessor.Preprocessor(\"data/original/alice_in_wonderland.txt\", \"eng\", False)\n",
    "cleaned_corpus = pp.process()\n",
    "#pp.split(cleaned_corpus)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[later editions continued as follows When the sands are all dry, he is gay as a lark, And will talk in contemptuous tones of the Shark, But, when the tide rises and sharks are around, His voice has a timid and tremulous sound.].\n",
      "[later editions continued as follows The Panther took pie-crust, and gravy, and meat, While the Owl had the dish as its share of the treat.\n",
      "Number of unique characters: 35\n",
      "Number of characters: 367\n",
      "dict_keys(['[', 'l', 'a', 't', 'e', 'r', ' ', 'd', 'i', 'o', 'n', 's', 'c', 'u', 'f', 'w', 'W', 'h', 'y', ',', 'g', 'k', 'A', 'm', 'p', 'S', 'B', 'H', 'v', '.', ']', 'T', 'P', '-', 'O'])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Subword segmentation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "import sentpiece\n",
    "sentpiece = reload(sentpiece)\n",
    "\n",
    "# Character level train vocabular size = 68\n",
    "# Character level test vocabular size = 68\n",
    "\n",
    "\n",
    "NUM_CHARS = 68\n",
    "TRAIN_DATA_PATH = \"data/processed/eng_test.txt\"\n",
    "MODEL_NAME = \"eng_model_chr\"\n",
    "TYPE = \"_test\"\n",
    "\n",
    "\n",
    "sentpiece.train_model(TRAIN_DATA_PATH, MODEL_NAME + TYPE, NUM_CHARS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "!mv $MODEL_NAME* spm_models/"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "SEG_DATA_FILE = \"eng_chr_test.txt\"\n",
    "\n",
    "\n",
    "sentpiece.segmentation(TRAIN_DATA_PATH, MODEL_NAME, SEG_DATA_FILE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train LM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "TRAIN_SEG_DATA_PATH = \"data/segmented/eng_chr_train.txt\"\n",
    "TEST_SEG_DATA_PATH = \"data/segmented/eng_chr_test.txt\"\n",
    "NUM_LAYERS = 40\n",
    "CLASS = 9999\n",
    "BPTT = 3\n",
    "\n",
    "\n",
    "!bash train_script.sh $TRAIN_SEG_DATA_PATH $TEST_SEG_DATA_PATH $NUM_LAYERS $CLASS $BPTT"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "rm: cannot remove 'rnnlm/models/model': No such file or directory\n",
      "rm: cannot remove 'rnnlm/models/model.output.txt': No such file or directory\n",
      "debug mode: 2\n",
      "train file: data/segmented/eng_chr_train.txt\n",
      "valid file: data/segmented/eng_chr_test.txt\n",
      "class size: 9999\n",
      "Hidden layer size: 40\n",
      "BPTT: 3\n",
      "Rand seed: 1\n",
      "rnnlm file: model\n",
      "Starting training using file data/segmented/eng_chr_train.txt\n",
      "Vocab size: 66\n",
      "Words in train file: 113143\n",
      "WARNING: number of classes exceeds vocabulary size!\n",
      "Iter:   0\tAlpha: 0.100000\t   TRAIN entropy: 3.1878    Words/sec: 424.3   VALID entropy: 3.2710\n",
      "Iter:   1\tAlpha: 0.100000\t   TRAIN entropy: 2.7086    Words/sec: 425.2   VALID entropy: 3.0899\n",
      "Iter:   2\tAlpha: 0.100000\t   TRAIN entropy: 2.5951    Words/sec: 429.8   VALID entropy: 3.0287\n",
      "Iter:   3\tAlpha: 0.100000\t   TRAIN entropy: 2.5427    Words/sec: 693.5   VALID entropy: 2.9219\n",
      "Iter:   4\tAlpha: 0.100000\t   TRAIN entropy: 2.5117    Words/sec: 908.4   VALID entropy: 2.8855\n",
      "Iter:   5\tAlpha: 0.100000\t   TRAIN entropy: 2.4897    Words/sec: 929.5   VALID entropy: 2.8901\n",
      "Iter:   6\tAlpha: 0.050000\t   TRAIN entropy: 2.4187    Words/sec: 931.0   VALID entropy: 2.7062\n",
      "Iter:   7\tAlpha: 0.025000\t   TRAIN entropy: 2.3692    Words/sec: 907.1   VALID entropy: 2.5925\n",
      "Iter:   8\tAlpha: 0.012500\t   TRAIN entropy: 2.3453    Words/sec: 887.3   VALID entropy: 2.5230\n",
      "Iter:   9\tAlpha: 0.006250\t   TRAIN entropy: 2.3330    Words/sec: 917.8   VALID entropy: 2.4863\n",
      "Iter:  10\tAlpha: 0.003125\t   TRAIN entropy: 2.3266    Words/sec: 895.9   VALID entropy: 2.4692\n",
      "Iter:  11\tAlpha: 0.001563\t   TRAIN entropy: 2.3231    Words/sec: 931.2   VALID entropy: 2.4615\n",
      "Iter:  12\tAlpha: 0.000781\t   TRAIN entropy: 2.3209    Words/sec: 975.5   VALID entropy: 2.4583\n",
      "\n",
      "real\t36m24.902s\n",
      "user\t36m20.026s\n",
      "sys\t0m0.236s\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "MODEL_OUTPUT = MODEL_NAME + \".output.txt\"\n",
    "\n",
    "!mv model $MODEL_NAME\n",
    "!mv model.output.txt $MODEL_OUTPUT\n",
    "\n",
    "!mv $MODEL_NAME $MODEL_OUTPUT rnnlm_models/"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data generation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "GEN_DATA_PATH = \"data/generated/eng_model_chr/\"\n",
    "MODEL_PATH = f\"./rnnlm_models/{MODEL_NAME}\"\n",
    "\n",
    "!bash gen_script.sh $GEN_DATA_PATH $MODEL_PATH"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "# Decoding generated texts\n",
    "\n",
    "for i in range(1, 8):\n",
    "    size = 10**i\n",
    "    sentpiece.desegmentation(f\"data/generated/eng_model_chr/{size}.txt\", MODEL_NAME + TYPE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bengali"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}