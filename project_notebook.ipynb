{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('snlp': venv)"
  },
  "interpreter": {
   "hash": "f0a7245247b824f88e8cdc9749c4ec8d41cad3b6de9273672e84a39a1c855c1d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# English - Alice in wonderland\n",
    "\n",
    "## Data preparation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from importlib import reload\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pavlem/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "import preprocessor\n",
    "preprocessor = reload(preprocessor)\n",
    "\n",
    "\n",
    "pp = preprocessor.Preprocessor(\"data/original/alice_in_wonderland.txt\", \"eng\", False)\n",
    "cleaned_corpus = pp.process()\n",
    "pp.split(cleaned_corpus)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[later editions continued as follows When the sands are all dry, he is gay as a lark, And will talk in contemptuous tones of the Shark, But, when the tide rises and sharks are around, His voice has a timid and tremulous sound.].\n",
      "[later editions continued as follows The Panther took pie-crust, and gravy, and meat, While the Owl had the dish as its share of the treat.\n",
      "Number of unique characters: 35\n",
      "Number of characters: 367\n",
      "dict_keys(['[', 'l', 'a', 't', 'e', 'r', ' ', 'd', 'i', 'o', 'n', 's', 'c', 'u', 'f', 'w', 'W', 'h', 'y', ',', 'g', 'k', 'A', 'm', 'p', 'S', 'B', 'H', 'v', '.', ']', 'T', 'P', '-', 'O'])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Subword segmentation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import sentpiece\n",
    "sentpiece = reload(sentpiece)\n",
    "\n",
    "# Character level train vocabular size = 68\n",
    "# Character level test vocabular size = 68\n",
    "\n",
    "\n",
    "NUM_CHARS = 2700\n",
    "TRAIN_DATA_PATH = \"data/processed/eng_test.txt\"\n",
    "MODEL_NAME = \"eng_model_lrg\"\n",
    "TYPE = \"_test\"\n",
    "\n",
    "\n",
    "sentpiece.train_model(TRAIN_DATA_PATH, MODEL_NAME + TYPE, NUM_CHARS)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/processed/eng_test.txt\n",
      "  input_format: \n",
      "  model_prefix: eng_model_lrg_test\n",
      "  model_type: BPE\n",
      "  vocab_size: 2700\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: data/processed/eng_test.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 272 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=29527\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=65\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 272 sentences.\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 272\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 1762\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=899 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=155 size=20 all=1005 active=940 piece=▁c\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=98 size=40 all=1226 active=1161 piece=▁in\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=76 size=60 all=1488 active=1423 piece='.\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=56 size=80 all=1652 active=1587 piece=ter\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=42 size=100 all=1774 active=1709 piece=ent\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=42 min_freq=2\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=34 size=120 all=1928 active=1134 piece=our\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=26 size=140 all=2025 active=1231 piece=ain\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=22 size=160 all=2071 active=1277 piece=▁Q\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=20 size=180 all=2135 active=1341 piece=▁what\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=18 size=200 all=2189 active=1395 piece=▁little\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=17 min_freq=2\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=16 size=220 all=2241 active=1053 piece=▁\"\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14 size=240 all=2294 active=1106 piece=mb\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=13 size=260 all=2375 active=1187 piece=ne\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12 size=280 all=2432 active=1244 piece=It\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=300 all=2486 active=1298 piece=ded\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=11 min_freq=2\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=320 all=2540 active=1053 piece=▁Soup\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10 size=340 all=2583 active=1096 piece=ther\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9 size=360 all=2615 active=1128 piece=▁L\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9 size=380 all=2638 active=1151 piece=▁tone\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8 size=400 all=2650 active=1163 piece=▁into\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=8 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7 size=420 all=2665 active=1016 piece=ear\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7 size=440 all=2715 active=1066 piece=▁sis\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6 size=460 all=2722 active=1073 piece=iz\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6 size=480 all=2767 active=1118 piece=▁we\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6 size=500 all=2784 active=1135 piece=▁back\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=6 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6 size=520 all=2779 active=996 piece=▁explain\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5 size=540 all=2804 active=1021 piece=ile\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5 size=560 all=2842 active=1059 piece=▁Liz\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5 size=580 all=2844 active=1061 piece=butter\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5 size=600 all=2837 active=1054 piece=▁evening\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=5 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=620 all=2849 active=1013 piece=ick\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=640 all=2889 active=1053 piece=OULD\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=660 all=2913 active=1077 piece=▁cro\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=680 all=2922 active=1086 piece=▁come\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=700 all=2913 active=1077 piece=iously\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=4 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=720 all=2908 active=994 piece=▁mouths\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=740 all=2893 active=979 piece=▁int"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "!mv $MODEL_NAME* spm_models/"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "SEG_DATA_FILE = \"eng_lrg_test.txt\"\n",
    "\n",
    "\n",
    "sentpiece.segmentation(TRAIN_DATA_PATH, MODEL_NAME + TYPE, SEG_DATA_FILE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train LM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "TRAIN_SEG_DATA_PATH = \"data/segmented/eng_lrg_train.txt\"\n",
    "TEST_SEG_DATA_PATH = \"data/segmented/eng_lrg_test.txt\"\n",
    "NUM_LAYERS = 40\n",
    "CLASS = 9999\n",
    "BPTT = 3\n",
    "\n",
    "\n",
    "!bash train_script.sh $TRAIN_SEG_DATA_PATH $TEST_SEG_DATA_PATH $NUM_LAYERS $CLASS $BPTT"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "rm: cannot remove 'rnnlm/models/model': No such file or directory\n",
      "rm: cannot remove 'rnnlm/models/model.output.txt': No such file or directory\n",
      "debug mode: 2\n",
      "train file: data/segmented/eng_lrg_train.txt\n",
      "valid file: data/segmented/eng_lrg_test.txt\n",
      "class size: 9999\n",
      "Hidden layer size: 40\n",
      "BPTT: 3\n",
      "Rand seed: 1\n",
      "rnnlm file: model\n",
      "Starting training using file data/segmented/eng_lrg_train.txt\n",
      "Vocab size: 2334\n",
      "Words in train file: 29151\n",
      "WARNING: number of classes exceeds vocabulary size!\n",
      "Iter:   0\tAlpha: 0.100000\t   TRAIN entropy: 9.0951    Words/sec: 839.9   VALID entropy: 8.3108\n",
      "Iter:   1\tAlpha: 0.100000\t   TRAIN entropy: 8.1197    Words/sec: 885.8   VALID entropy: 7.8704\n",
      "Iter:   2\tAlpha: 0.100000\t   TRAIN entropy: 7.6205    Words/sec: 946.2   VALID entropy: 7.7488\n",
      "Iter:   3\tAlpha: 0.100000\t   TRAIN entropy: 7.2727    Words/sec: 962.1   VALID entropy: 7.5793\n",
      "Iter:   4\tAlpha: 0.100000\t   TRAIN entropy: 7.0079    Words/sec: 982.4   VALID entropy: 7.3153\n",
      "Iter:   5\tAlpha: 0.100000\t   TRAIN entropy: 6.7887    Words/sec: 984.7   VALID entropy: 7.1944\n",
      "Iter:   6\tAlpha: 0.100000\t   TRAIN entropy: 6.6022    Words/sec: 868.7   VALID entropy: 7.2289\n",
      "Iter:   7\tAlpha: 0.050000\t   TRAIN entropy: 6.4945    Words/sec: 995.1   VALID entropy: 6.8820\n",
      "Iter:   8\tAlpha: 0.025000\t   TRAIN entropy: 6.3232    Words/sec: 996.7   VALID entropy: 6.6301\n",
      "Iter:   9\tAlpha: 0.012500\t   TRAIN entropy: 6.2459    Words/sec: 999.4   VALID entropy: 6.4352\n",
      "Iter:  10\tAlpha: 0.006250\t   TRAIN entropy: 6.2139    Words/sec: 993.8   VALID entropy: 6.2004\n",
      "Iter:  11\tAlpha: 0.003125\t   TRAIN entropy: 6.1955    Words/sec: 996.1   VALID entropy: 6.0757\n",
      "Iter:  12\tAlpha: 0.001563\t   TRAIN entropy: 6.1857    Words/sec: 900.5   VALID entropy: 6.0305\n",
      "Iter:  13\tAlpha: 0.000781\t   TRAIN entropy: 6.1795    Words/sec: 889.2   VALID entropy: 6.0140\n",
      "\n",
      "real\t7m37.054s\n",
      "user\t7m36.188s\n",
      "sys\t0m0.213s\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "MODEL_OUTPUT = MODEL_NAME + \".output.txt\"\n",
    "\n",
    "!mv model $MODEL_NAME\n",
    "!mv model.output.txt $MODEL_OUTPUT\n",
    "\n",
    "!mv $MODEL_NAME $MODEL_OUTPUT rnnlm_models/"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data generation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "GEN_DATA_PATH = \"data/generated/eng_model_chr/\"\n",
    "MODEL_PATH = f\"./rnnlm_models/{MODEL_NAME}\"\n",
    "\n",
    "!bash gen_script.sh $GEN_DATA_PATH $MODEL_PATH"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "# Decoding generated texts\n",
    "\n",
    "for i in range(1, 8):\n",
    "    size = 10**i\n",
    "    sentpiece.desegmentation(f\"data/generated/eng_model_chr/{size}.txt\", MODEL_NAME + TYPE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bengali"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}