{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English - Alice in wonderland\n",
    "\n",
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessor\n",
    "preprocessor = reload(preprocessor)\n",
    "\n",
    "\n",
    "pp = preprocessor.Preprocessor(\"data/original/alice_in_wonderland.txt\", \"eng\")\n",
    "cleaned_corpus = pp.process()\n",
    "pp.split(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subword segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentpiece\n",
    "sentpiece = reload(sentpiece)\n",
    "\n",
    "\n",
    "NUM_CHARS = 100\n",
    "TRAIN_DATA_PATH = \"data/processed/eng_train.txt\"\n",
    "MODEL_NAME = \"eng_model_sml\"\n",
    "SEG_DATA_PATH = \"eng_sml.txt\"\n",
    "\n",
    "\n",
    "sentpiece.train_model(TRAIN_DATA_PATH, MODEL_NAME, NUM_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv $MODEL_NAME* spm_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentpiece.segmentation(TRAIN_DATA_PATH, MODEL_NAME, SEG_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'rnnlm/models/model': No such file or directory\n",
      "rm: cannot remove 'rnnlm/models/model.output.txt': No such file or directory\n",
      "debug mode: 2\n",
      "train file: data/segmented/eng_sml.txt\n",
      "valid file: data/processed/eng_test.txt\n",
      "class size: 9999\n",
      "Hidden layer size: 40\n",
      "BPTT: 3\n",
      "Rand seed: 1\n",
      "rnnlm file: model\n",
      "Starting training using file data/segmented/eng_sml.txt\n",
      "Vocab size: 98\n",
      "Words in train file: 70003\n",
      "WARNING: number of classes exceeds vocabulary size!\n",
      "Iter:   0\tAlpha: 0.100000\t   TRAIN entropy: 4.9914    Words/sec: 1226.2   VALID entropy: 11.5537\n",
      "Iter:   1\tAlpha: 0.100000\t   TRAIN entropy: 4.1096    Words/sec: 1382.2   VALID entropy: 12.4125\n",
      "Iter:   2\tAlpha: 0.050000\t   TRAIN entropy: 4.0804    Words/sec: 1380.0   VALID entropy: 12.5771\n",
      "\n",
      "real\t2m43.524s\n",
      "user\t2m42.983s\n",
      "sys\t0m0.176s\n"
     ]
    }
   ],
   "source": [
    "TRAIN_SEG_DATA_PATH = \"data/segmented/eng_sml.txt\"\n",
    "NUM_LAYERS = 40\n",
    "CLASS = 9999\n",
    "BPTT = 3\n",
    "\n",
    "\n",
    "!bash train_script.sh $TRAIN_SEG_DATA_PATH \"data/processed/eng_test.txt\" $NUM_LAYERS $CLASS $BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = MODEL_NAME + \".output.txt\"\n",
    "\n",
    "!mv model $MODEL_NAME\n",
    "!mv model.output.txt $MODEL_OUTPUT\n",
    "\n",
    "!mv $MODEL_NAME $MODEL_OUTPUT rnnlm_models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "GEN_DATA_PATH = \"data/generated/eng_model_sml/\"\n",
    "MODEL_PATH = f\"./rnnlm_models/{MODEL_NAME}\"\n",
    "\n",
    "!bash gen_script.sh $GEN_DATA_PATH $MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding generated texts\n",
    "\n",
    "sentpiece.desegmentation(\"data/generated/eng_model_sml/100.txt\", \"eng_model_chr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bengali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10861 2716\n"
     ]
    }
   ],
   "source": [
    "import preprocessor\n",
    "preprocessor = reload(preprocessor)\n",
    "\n",
    "\n",
    "pp = preprocessor.Preprocessor(\"data/original/bengali_corpus.txt\", \"bng\")\n",
    "cleaned_corpus = pp.process()\n",
    "pp.split(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subword segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentpiece\n",
    "sentpiece = reload(sentpiece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# character-level\n",
    "TRAIN_DATA_PATH = \"data/processed/bng_train.txt\"\n",
    "TEST_DATA_PATH = \"data/processed/bng_test.txt\"\n",
    "models, test_models = [], []\n",
    "seg_train_path, seg_test_path = [], []\n",
    "start=50\n",
    "stop=100\n",
    "step=50\n",
    "\n",
    "for vocab_size in range(start, stop, step):\n",
    "    MODEL_NAME = \"bng_train_\"+str(vocab_size)\n",
    "    SEG_DATA_PATH = f\"bng_tr_chr_{str(vocab_size)}.txt\"\n",
    "    sentpiece.train_model(TRAIN_DATA_PATH, MODEL_NAME, vocab_size, lang=\"bng\")\n",
    "    models.append(MODEL_NAME)\n",
    "    seg_train_path.append(SEG_DATA_PATH)\n",
    "    \n",
    "for vocab_size in range(start, stop, step):\n",
    "    MODEL_NAME = \"bng_test_\"+str(vocab_size)\n",
    "    SEG_DATA_PATH = f\"bng_te_chr_{str(vocab_size)}.txt\"\n",
    "    sentpiece.train_model(TEST_DATA_PATH, MODEL_NAME, vocab_size, lang=\"bng\")\n",
    "    test_models.append(MODEL_NAME)\n",
    "    seg_test_path.append(SEG_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, glob, os\n",
    "\n",
    "for model in models+test_models:\n",
    "    for file in glob.glob(f'{model}*'):\n",
    "        cwd = os.getcwd() \n",
    "        src = cwd\n",
    "        dst = cwd + \"/spm_models/\"\n",
    "        shutil.move(os.path.join(src, file), os.path.join(dst, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, seg_path in zip(models,seg_train_path):\n",
    "    sentpiece.segmentation(TRAIN_DATA_PATH, model, seg_path)\n",
    "\n",
    "for model, seg_path in zip(test_models,seg_test_path):\n",
    "    sentpiece.segmentation(TEST_DATA_PATH, model, seg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, seg_path in zip(models,seg_train_path):\n",
    "    print(seg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "NUM_LAYERS = [60,70]\n",
    "CLASS = [50,6000,7000,8000,9000,9999]\n",
    "BPTT = [3,4]\n",
    "\n",
    "dir_path=\"data/segmented/\"\n",
    "\n",
    "for train_file, test_file in zip(seg_train_path, seg_test_path):\n",
    "    for hid in NUM_LAYERS:\n",
    "        for cl in CLASS:\n",
    "            for bp in BPTT:\n",
    "                fname=\"hd\"+str(hid)+\"_cl\"+str(cl)+\"_\"+str(bp)+\"_\"+train_file[train_file.rfind(\"/\")+1:train_file.rfind('.')]\n",
    "                print(fname)\n",
    "                args = ['bash', 'train_script.sh', dir_path+train_file, dir_path+test_file, fname, str(hid), str(cl), str(bp)]\n",
    "                p = subprocess.run(args)\n",
    "                print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_DATA_PATH = \"data/generated/hd70_cl6000_3_bng_tr_chr_50/\"\n",
    "MODEL_PATH = f\"./rnnlm_models/hd70_cl6000_3_bng_tr_chr_50\"\n",
    "\n",
    "from pathlib import Path\n",
    "Path(GEN_DATA_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "!bash gen_script.sh $GEN_DATA_PATH $MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding generated texts\n",
    "\n",
    "sentpiece.desegmentation(\"data/generated/hd70_cl6000_3_bng_tr_chr_50/100.txt\", \"bng_train_50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOV comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_vocab(path):\n",
    "    vocab=[]\n",
    "    with open(path) as f:\n",
    "        data = f.read()\n",
    "        vocab = Counter(data.split())\n",
    "        return vocab\n",
    "\n",
    "TRAIN_DATA_PATH = \"data/processed/bng_train.txt\"\n",
    "TEST_DATA_PATH = \"data/processed/bng_test.txt\"\n",
    "train_vocab = get_vocab(TRAIN_DATA_PATH)\n",
    "test_vocab = get_vocab(TEST_DATA_PATH)\n",
    "print(len(train_vocab), len(test_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_oov_rate(train_vocab, test_vocab):\n",
    "    oov_words = list(test_vocab.keys()-train_vocab.keys())\n",
    "    count_oov = 0\n",
    "    for k, v in test_vocab.items():\n",
    "        if k in oov_words:\n",
    "            count_oov += v\n",
    "            \n",
    "    oov_rate = count_oov / sum(test_vocab.values())\n",
    "    return oov_rate\n",
    "\n",
    "compute_oov_rate(train_vocab, test_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_vocab(train_vocab, add_vocab):\n",
    "    #print(len(train_vocab))\n",
    "    for k, v in add_vocab.items():\n",
    "        if k not in train_vocab:\n",
    "            train_vocab.update({k:v})\n",
    "        else:\n",
    "            train_vocab.update({k:train_vocab[k]+v})\n",
    "    #print(len(train_vocab))\n",
    "    return train_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char-level baselines \n",
    "dir_path=\"data/generated/bng_chr_50/\"\n",
    "oov_rates = {}\n",
    "for i in range(1,8):\n",
    "    file=10**i\n",
    "    print(file)\n",
    "    #sentpiece.desegmentation(dir_path+str(file)+\".txt\", \"bng_train_50\")\n",
    "    add_vocab=get_vocab(dir_path+str(file)+\"_desegmented.txt\")\n",
    "    oov_rates[file] = compute_oov_rate(augment_vocab(train_vocab, add_vocab), test_vocab)\n",
    "print(oov_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_oov_rates(oov_rates) -> None:\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.loglog(list(oov_rates.keys()), list(oov_rates.values()))\n",
    "    ax.set_xlabel(\"vocab size\")\n",
    "    ax.set_ylabel(\"OOV rate\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_oov_rates(oov_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
